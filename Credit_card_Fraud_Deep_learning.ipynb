{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNqzmRxN9mRNrBAO4RImKPE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NzimbaEnvoy/Fraud_Detection-Masters-Project-/blob/main/Credit_card_Fraud_Deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWRFLM0mzgu9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing Libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, f1_score\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM, TimeDistributed, BatchNormalization\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import itertools\n",
        "import shap\n"
      ],
      "metadata": {
        "id": "wSz41VYPA4Zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the files\n",
        "base_path = \"/content/drive/MyDrive/Thesis Project\"\n",
        "\n",
        "train_path = os.path.join(base_path, \"train_data_Credit Card.csv\")\n",
        "test_path = os.path.join(base_path, \"test_data_Credit Card.csv\")\n",
        "\n",
        "# Loading data\n",
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)"
      ],
      "metadata": {
        "id": "dotwIAKVA4b5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "id": "6nDNOv-bBxj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = train_df.drop(\"Class\", axis=1)\n",
        "y_train = train_df[\"Class\"]\n",
        "\n",
        "X_test = test_df.drop(\"Class\", axis=1)\n",
        "y_test = test_df[\"Class\"]\n"
      ],
      "metadata": {
        "id": "vamfYqAUA4eP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN BASELINE"
      ],
      "metadata": {
        "id": "FHsw3vytBgCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshaping for CNN input\n",
        "X_train_cnn = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test_cnn  = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))"
      ],
      "metadata": {
        "id": "OoM3nypMA4hH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyper Parameter Tuning"
      ],
      "metadata": {
        "id": "4HHgzQEkBo8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, BatchNormalization, MaxPooling1D, AveragePooling1D, Dropout, Flatten, GlobalAveragePooling1D, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def build_cnn_model(filters, kernel_size, dropout_rate, dense_units, learning_rate,\n",
        "                    blocks=2, pool_type='max', l2_reg=0.0, use_gap=False, dilation_rate=1):\n",
        "    \"\"\"\n",
        "    Deeper/tunable 1D-CNN with 2 or 3 conv blocks, BN, pooling, dropout, and GAP/Flatten head.\n",
        "    - blocks: 2 or 3 conv blocks\n",
        "    - pool_type: 'max' or 'avg'\n",
        "    - l2_reg: L2 regularization strength for conv kernels\n",
        "    - use_gap: True -> GlobalAveragePooling1D, False -> Flatten\n",
        "    - dilation_rate: 1 or 2 for dilated convs\n",
        "    \"\"\"\n",
        "    Pool = MaxPooling1D if pool_type == 'max' else AveragePooling1D\n",
        "    reg = l2(l2_reg) if l2_reg > 0 else None\n",
        "    head_dropout = min(max(dropout_rate + 0.1, 0.0), 0.8)\n",
        "\n",
        "    model = Sequential([\n",
        "        # ---- Block 1 ----\n",
        "        Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', padding='same',\n",
        "               dilation_rate=dilation_rate, kernel_regularizer=reg, input_shape=(X_train_cnn.shape[1], 1)),\n",
        "        BatchNormalization(),\n",
        "        Conv1D(filters=filters * 2, kernel_size=kernel_size, activation='relu', padding='same',\n",
        "               dilation_rate=dilation_rate, kernel_regularizer=reg),\n",
        "        BatchNormalization(),\n",
        "        Pool(pool_size=2),\n",
        "        Dropout(dropout_rate),\n",
        "\n",
        "        # ---- Block 2 ----\n",
        "        Conv1D(filters=filters * 4, kernel_size=kernel_size, activation='relu', padding='same',\n",
        "               dilation_rate=dilation_rate, kernel_regularizer=reg),\n",
        "        BatchNormalization(),\n",
        "        Conv1D(filters=filters * 4, kernel_size=kernel_size, activation='relu', padding='same',\n",
        "               dilation_rate=dilation_rate, kernel_regularizer=reg),\n",
        "        BatchNormalization(),\n",
        "        Pool(pool_size=2),\n",
        "        Dropout(dropout_rate),\n",
        "    ])\n",
        "\n",
        "    if blocks == 3:\n",
        "        # ---- Block 3 ----\n",
        "        model.add(Conv1D(filters=filters * 8, kernel_size=kernel_size, activation='relu', padding='same',\n",
        "                         dilation_rate=dilation_rate, kernel_regularizer=reg))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Conv1D(filters=filters * 8, kernel_size=kernel_size, activation='relu', padding='same',\n",
        "                         dilation_rate=dilation_rate, kernel_regularizer=reg))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Pool(pool_size=2))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # ---- Head ----\n",
        "    if use_gap:\n",
        "        model.add(GlobalAveragePooling1D())\n",
        "    else:\n",
        "        model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(dense_units, activation='relu'))\n",
        "    model.add(Dropout(head_dropout))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Expanded grid\n",
        "param_grid = {\n",
        "    'filters': [16, 32, 48, 64],\n",
        "    'kernel_size': [3, 5],\n",
        "    'dropout_rate': [0.2, 0.3, 0.4],\n",
        "    'dense_units': [64, 128, 256],\n",
        "    'learning_rate': [1e-4, 5e-4, 1e-3],\n",
        "    'blocks': [2, 3, 4],\n",
        "    'pool_type': ['max', 'avg'],\n",
        "    'l2_reg': [0.0, 1e-4, 1e-3],\n",
        "    'use_gap': [False, True],\n",
        "    'dilation_rate': [1, 2, 3],\n",
        "\n",
        "    # Training hyperparams (tuned in the loop)\n",
        "    'batch_size': [64, 128, 256]\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n",
        "\n",
        "# Loop over all combinations\n",
        "keys = list(param_grid.keys())\n",
        "for values in itertools.product(*param_grid.values()):\n",
        "    params = dict(zip(keys, values))\n",
        "    print(f\"Testing: {params}\")\n",
        "\n",
        "    # Building with architecture params\n",
        "    model = build_cnn_model(\n",
        "        filters=params['filters'],\n",
        "        kernel_size=params['kernel_size'],\n",
        "        dropout_rate=params['dropout_rate'],\n",
        "        dense_units=params['dense_units'],\n",
        "        learning_rate=params['learning_rate'],\n",
        "        blocks=params['blocks'],\n",
        "        pool_type=params['pool_type'],\n",
        "        l2_reg=params['l2_reg'],\n",
        "        use_gap=params['use_gap'],\n",
        "        dilation_rate=params['dilation_rate']\n",
        "    )\n",
        "\n",
        "    # Training with training params from grid\n",
        "    history = model.fit(\n",
        "        X_train_cnn, y_train,\n",
        "        validation_split=0.2,\n",
        "        epochs=params['epochs'],\n",
        "        batch_size=params['batch_size'],\n",
        "        verbose=0,\n",
        "        callbacks=[early_stop]\n",
        "    )\n",
        "\n",
        "    # Evaluating\n",
        "    y_pred_probs = model.predict(X_test_cnn, verbose=0).flatten()\n",
        "    y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    results.append((params, acc))\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Picking the best\n",
        "best_params, best_score = max(results, key=lambda x: x[1])\n",
        "print(\"\\nBest Parameters for CNN:\")\n",
        "print(best_params)\n",
        "print(f\"Best Accuracy: {best_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "G4H_otrQA4jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODELLING**"
      ],
      "metadata": {
        "id": "NB4liIRGCDaq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN BASELINE"
      ],
      "metadata": {
        "id": "T9vVJEMre6XM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining early stopping\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "#CNN architecture\n",
        "cnn_baseline = Sequential([\n",
        "    # Block 1\n",
        "    Conv1D(32, 3, activation='relu', padding='same', input_shape=(X_train_cnn.shape[1], 1)),\n",
        "    BatchNormalization(),\n",
        "    Conv1D(64, 3, activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    # Block 2\n",
        "    Conv1D(128, 3, activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Conv1D(128, 3, activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.4),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n"
      ],
      "metadata": {
        "id": "P9-o1nKnA4n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling with extended metrics\n",
        "cnn_baseline.compile(\n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
        ")\n",
        "\n",
        "\n",
        "# Training with validation and early stopping\n",
        "cnn_baseline.fit(\n",
        "    X_train_cnn,\n",
        "    y_train,\n",
        "    epochs=30,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Predicting & Evaluating\n",
        "y_pred_probs_baseline = cnn_baseline.predict(X_test_cnn).flatten()\n",
        "y_pred_baseline = (y_pred_probs_baseline > 0.5).astype(int)\n",
        "\n",
        "cm_baseline = confusion_matrix(y_test, y_pred_baseline)\n",
        "roc_baseline = roc_auc_score(y_test, y_pred_probs_baseline)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm_baseline)\n",
        "print(f\"AUC: {roc_baseline:.4f}\")"
      ],
      "metadata": {
        "id": "jTWYtEJlA4qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the trained model\n",
        "model_dir = \"/content/drive/My Drive/Thesis Project\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "cnn_baseline.save(os.path.join(model_dir, \"cnn_baseline_model.h5\"))\n",
        "print(\"Model saved to:\", os.path.join(model_dir, \"cnn_baseline_model.h5\"))"
      ],
      "metadata": {
        "id": "04xdfAvi8joc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "REALTIME INFERENCES"
      ],
      "metadata": {
        "id": "m3ManX4S8iTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#Loading the CNN model\n",
        "model_path = \"/content/drive/My Drive/Thesis Project/cnn_baseline_model.h5\"\n",
        "cnn_baseline = load_model(model_path)\n",
        "\n",
        "#Converting test data to NumPy arrays\n",
        "X_test_np = X_test.values\n",
        "y_test_np = y_test.values\n",
        "\n",
        "#Fraud-only pool (true positives only)\n",
        "fraud_idx = np.where(y_test_np == 1)[0]\n",
        "fraud_pool_X = X_test_np[fraud_idx]\n",
        "assert fraud_pool_X.shape[0] > 0, \"No fraud cases found in test set!\"\n",
        "\n",
        "#Simulation settings\n",
        "domain_name  = \"credit-card\"\n",
        "lambda_fraud = 50.16\n",
        "runs         = 10000\n",
        "threshold    = 0.5\n",
        "rng = np.random.default_rng(42)\n",
        "\n",
        "#Monte Carlo simulation\n",
        "arrivals_F   = rng.poisson(lam=lambda_fraud, size=runs)\n",
        "tp_vec       = np.zeros(runs, dtype=int)\n",
        "det_rate_pct = np.full(runs, np.nan)\n",
        "\n",
        "for r in range(runs):\n",
        "    F = arrivals_F[r]\n",
        "    if F == 0:\n",
        "        continue\n",
        "    s_idx = rng.choice(fraud_pool_X.shape[0], size=F, replace=True)\n",
        "    X_r   = fraud_pool_X[s_idx]\n",
        "\n",
        "    # Predicting probabilities with CNN\n",
        "    p_r   = cnn_baseline.predict(X_r, verbose=0).flatten()\n",
        "    yhat  = (p_r >= threshold).astype(int)\n",
        "\n",
        "    TP    = np.sum(yhat == 1)\n",
        "    tp_vec[r]       = TP\n",
        "    det_rate_pct[r] = 100 * TP / F\n",
        "\n",
        "# Wrapping into DataFrame\n",
        "mc_df = pd.DataFrame({\n",
        "    \"run\": np.arange(1, runs+1),\n",
        "    \"fraud_arrivals\": arrivals_F,\n",
        "    \"tp\": tp_vec,\n",
        "    \"detection_pct\": det_rate_pct\n",
        "})\n",
        "\n",
        "plot_df = mc_df.dropna(subset=[\"detection_pct\"])"
      ],
      "metadata": {
        "id": "ToeGOD5Y8g_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Summary stats\n",
        "summary_table = pd.DataFrame([{\n",
        "    \"model\": \"CNN (Baseline)\",\n",
        "    \"threshold\": threshold,\n",
        "    \"mean_detection_pct\": plot_df[\"detection_pct\"].mean(),\n",
        "    \"median_detection_pct\": plot_df[\"detection_pct\"].median(),\n",
        "    \"p05_detection_pct\": np.percentile(plot_df[\"detection_pct\"], 5),\n",
        "    \"p95_detection_pct\": np.percentile(plot_df[\"detection_pct\"], 95),\n",
        "    \"mean_tp_per_sec\": lambda_fraud * plot_df[\"detection_pct\"].mean()/100,\n",
        "    \"p05_tp_per_sec\": lambda_fraud * np.percentile(plot_df[\"detection_pct\"], 5)/100,\n",
        "    \"p95_tp_per_sec\": lambda_fraud * np.percentile(plot_df[\"detection_pct\"], 95)/100\n",
        "}])\n",
        "\n",
        "print(summary_table)"
      ],
      "metadata": {
        "id": "0g7_303J8hDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checks\n",
        "print(\"Total runs:\", runs)\n",
        "print(\"Runs plotted (non-NA):\", len(plot_df))"
      ],
      "metadata": {
        "id": "tHpsw-ic6X5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting histogram\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(plot_df[\"detection_pct\"], bins=40, kde=False, color=\"blue\", alpha=0.6)\n",
        "plt.title(f\"Detection rate per second — Monte Carlo ({domain_name})\")\n",
        "plt.xlabel(\"Detection Rate per second (%)\")\n",
        "plt.ylabel(\"Count of runs\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZG5KKwz98hJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting ECDF\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.ecdfplot(plot_df[\"detection_pct\"], color=\"blue\", linewidth=1.5)\n",
        "plt.title(f\"ECDF of per-second detection — Monte Carlo ({domain_name})\")\n",
        "plt.xlabel(\"Detection Rate per second (%)\")\n",
        "plt.ylabel(\"ECDF\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l2O9_Jhj5kyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9TMuUghHVBzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# 1) Load model + test data\n",
        "model_path = \"/content/drive/My Drive/Thesis Project/cnn_baseline_model.h5\"\n",
        "cnn_baseline = load_model(model_path)\n",
        "\n",
        "# If X_test / y_test are pandas objects:\n",
        "X_test_np = X_test.values if hasattr(X_test, \"values\") else np.asarray(X_test)\n",
        "y_test_np = y_test.values if hasattr(y_test, \"values\") else np.asarray(y_test)\n",
        "\n",
        "# Ensure labels are {0,1} integers\n",
        "y_test_np = y_test_np.astype(int)\n",
        "assert set(np.unique(y_test_np)).issubset({0,1}), \"y_test must be binary {0,1}\"\n",
        "assert X_test_np.shape[0] == y_test_np.shape[0]\n",
        "\n",
        "# 2) Simulation settings\n",
        "lambda_per_sec = 50.16\n",
        "runs           = 10000\n",
        "thr_cnn        = 0.50\n",
        "rng            = np.random.default_rng(123)\n"
      ],
      "metadata": {
        "id": "YQC2VW0fVB1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Probability helper\n",
        "def predict_prob_keras(model, X_batch, batch_size=4096):\n",
        "    \"\"\"\n",
        "    Returns p(y=1) for a batch using a Keras model.\n",
        "    Works whether model outputs shape (n,1) or (n,).\n",
        "    \"\"\"\n",
        "    p = model.predict(X_batch, batch_size=batch_size, verbose=0)\n",
        "    p = np.asarray(p).reshape(-1)\n",
        "    return p"
      ],
      "metadata": {
        "id": "aqr_MU2yVB4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Windowed Monte Carlo simulator\n",
        "def simulate_realtime_fullpool_keras(model, threshold, lambda_rate_per_sec, runs,\n",
        "                                     X_pool, y_pool, window_secs=1):\n",
        "    \"\"\"\n",
        "    Each run is a time window of length `window_secs`.\n",
        "    We draw arrivals ~ Poisson(lambda_rate_per_sec * window_secs),\n",
        "    sample that many rows with replacement from the test pool,\n",
        "    score them, threshold to get yhat, and compute per-window metrics.\n",
        "    \"\"\"\n",
        "    N = X_pool.shape[0]\n",
        "    arrivals = rng.poisson(lam=lambda_rate_per_sec * window_secs, size=runs)\n",
        "\n",
        "    TP = np.zeros(runs, dtype=np.int32)\n",
        "    FP = np.zeros(runs, dtype=np.int32)\n",
        "    TN = np.zeros(runs, dtype=np.int32)\n",
        "    FN = np.zeros(runs, dtype=np.int32)\n",
        "\n",
        "    precision = np.full(runs, np.nan, dtype=float)\n",
        "    recall    = np.full(runs, np.nan, dtype=float)\n",
        "    accuracy  = np.full(runs, np.nan, dtype=float)\n",
        "    f1        = np.full(runs, np.nan, dtype=float)\n",
        "    auc_vec   = np.full(runs, np.nan, dtype=float)\n",
        "\n",
        "    for r in range(runs):\n",
        "        F = arrivals[r]\n",
        "        if F == 0:\n",
        "            continue\n",
        "\n",
        "        idx = rng.integers(0, N, size=F, dtype=np.int64)\n",
        "        X_r = X_pool[idx]\n",
        "        y_r = y_pool[idx]\n",
        "\n",
        "        p_r = predict_prob_keras(model, X_r)\n",
        "        yhat = (p_r >= threshold).astype(np.int32)\n",
        "\n",
        "        tp = np.sum((yhat == 1) & (y_r == 1))\n",
        "        fp = np.sum((yhat == 1) & (y_r == 0))\n",
        "        tn = np.sum((yhat == 0) & (y_r == 0))\n",
        "        fn = np.sum((yhat == 0) & (y_r == 1))\n",
        "\n",
        "        TP[r], FP[r], TN[r], FN[r] = tp, fp, tn, fn\n",
        "\n",
        "        denom_p = tp + fp\n",
        "        denom_r = tp + fn\n",
        "        if denom_p > 0:\n",
        "            precision[r] = tp / denom_p\n",
        "        if denom_r > 0:\n",
        "            recall[r] = tp / denom_r\n",
        "\n",
        "        accuracy[r] = (tp + tn) / F\n",
        "        if not np.isnan(precision[r]) and not np.isnan(recall[r]) and (precision[r] + recall[r]) > 0:\n",
        "            f1[r] = 2 * precision[r] * recall[r] / (precision[r] + recall[r])\n",
        "\n",
        "        # AUC only if both classes present in the window\n",
        "        if (y_r.min() == 0) and (y_r.max() == 1):\n",
        "            try:\n",
        "                auc_vec[r] = roc_auc_score(y_r, p_r)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    mc_df = pd.DataFrame({\n",
        "        \"run\": np.arange(1, runs+1, dtype=np.int32),\n",
        "        \"window_secs\": window_secs,\n",
        "        \"arrivals\": arrivals,\n",
        "        \"TP\": TP, \"FP\": FP, \"TN\": TN, \"FN\": FN,\n",
        "        \"precision\": precision, \"recall\": recall, \"accuracy\": accuracy, \"f1\": f1,\n",
        "        \"auc\": auc_vec\n",
        "    })\n",
        "\n",
        "    # Summary table\n",
        "    summary = {\n",
        "        \"runs\": runs,\n",
        "        \"window_secs\": window_secs,\n",
        "        \"runs_with_tx\": int(np.sum(arrivals > 0)),\n",
        "        \"mean_TP\": float(np.nanmean(TP)),\n",
        "        \"mean_FP\": float(np.nanmean(FP)),\n",
        "        \"mean_TN\": float(np.nanmean(TN)),\n",
        "        \"mean_FN\": float(np.nanmean(FN)),\n",
        "        \"mean_precision\": float(np.nanmean(precision)),\n",
        "        \"mean_recall\": float(np.nanmean(recall)),\n",
        "        \"mean_accuracy\": float(np.nanmean(accuracy)),\n",
        "        \"mean_f1\": float(np.nanmean(f1)),\n",
        "        \"mean_auc\": float(np.nanmean(auc_vec)),\n",
        "        \"median_auc\": float(np.nanmedian(auc_vec)),\n",
        "        \"n_auc_runs\": int(np.sum(~np.isnan(auc_vec))),\n",
        "    }\n",
        "    summary_df = pd.DataFrame([summary])\n",
        "    return {\"mc_df\": mc_df, \"summary\": summary_df}"
      ],
      "metadata": {
        "id": "l_VORGOwVB6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KRo-jF9lVB9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run the CNN\n",
        "res_cnn_1s  = simulate_realtime_fullpool_keras(\n",
        "    model=cnn_baseline, threshold=thr_cnn, lambda_rate_per_sec=lambda_per_sec,\n",
        "    runs=runs, X_pool=X_test_np, y_pool=y_test_np, window_secs=1\n",
        ")\n",
        "res_cnn_10s = simulate_realtime_fullpool_keras(\n",
        "    model=cnn_baseline, threshold=thr_cnn, lambda_rate_per_sec=lambda_per_sec,\n",
        "    runs=runs, X_pool=X_test_np, y_pool=y_test_np, window_secs=10\n",
        ")"
      ],
      "metadata": {
        "id": "Q4fl8ESnVB_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compare summaries\n",
        "summary_table = pd.concat([\n",
        "    res_cnn_1s[\"summary\"].assign(model=\"CNN (Keras)\", time_unit=\"1 sec\", threshold=thr_cnn),\n",
        "    res_cnn_10s[\"summary\"].assign(model=\"CNN (Keras)\", time_unit=\"10 sec\", threshold=thr_cnn),\n",
        "], ignore_index=True)[[\n",
        "    \"model\",\"time_unit\",\n",
        "    \"mean_accuracy\",\"mean_precision\",\"mean_recall\",\"mean_f1\",\n",
        "    \"mean_auc\"\n",
        "]]\n",
        "\n",
        "print(summary_table.to_string(index=False))"
      ],
      "metadata": {
        "id": "pR5GU2XjVCCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6EROhr5zVCEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SHAP ANALYSIS"
      ],
      "metadata": {
        "id": "P266GltuCTiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = X_train.columns.tolist()\n",
        "\n",
        "background = X_train_cnn[np.random.choice(X_train_cnn.shape[0], 200, replace=False)]\n",
        "X_explain = X_test_cnn[np.random.choice(X_test_cnn.shape[0], 100, replace=False)]\n",
        "\n",
        "# SHAP KernelExplainer\n",
        "background_flat = background.reshape(background.shape[0], -1)\n",
        "X_explain_flat = X_explain.reshape(X_explain.shape[0], -1)\n",
        "\n",
        "#model prediction wrapper\n",
        "def cnn_predict(x_flat):\n",
        "    x_reshaped = x_flat.reshape((-1, X_train_cnn.shape[1], 1))\n",
        "    return cnn_baseline.predict(x_reshaped).flatten()\n",
        "\n",
        "#SHAP explainer\n",
        "explainer = shap.KernelExplainer(cnn_predict, background_flat)\n",
        "\n",
        "#Computing SHAP values\n",
        "shap_values = explainer.shap_values(X_explain_flat)\n"
      ],
      "metadata": {
        "id": "Ypl-9fvZA4s1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7. Plot with real feature names ---\n",
        "plt.figure(figsize=(5, 5))  # Width=10, Height=5 inches\n",
        "shap.summary_plot(\n",
        "    shap_values,\n",
        "    X_explain_flat,\n",
        "    feature_names=feature_names,\n",
        "    plot_type=\"dot\",\n",
        "    show=False  # Prevent SHAP from auto-displaying\n",
        ")\n",
        "plt.title(\"SHAP Summary Plot-CNN (Baseline)\", fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        " #Save the figure\n",
        "plt.savefig(\"SHAP Summary Plot-CNN (Baseline).png\", dpi=300, bbox_inches='tight')\n"
      ],
      "metadata": {
        "id": "WAIX0IBjA4u-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SMOTE + CNN"
      ],
      "metadata": {
        "id": "uO8yjv2oCt59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "X_train_smote_cnn = X_train_smote.values.reshape((X_train_smote.shape[0], X_train_smote.shape[1], 1))"
      ],
      "metadata": {
        "id": "PwcCDbsIA4xU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define early stopping\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "#CNN architecture for SMOTE\n",
        "cnn_smote = Sequential([\n",
        "    # Block 1\n",
        "    Conv1D(32, 3, activation='relu', padding='same', input_shape=(X_train_smote_cnn.shape[1], 1)),\n",
        "    BatchNormalization(),\n",
        "    Conv1D(64, 3, activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    # Block 2\n",
        "    Conv1D(128, 3, activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Conv1D(128, 3, activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.4),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "VhekdmTTA4zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling model with same optimizer and metrics\n",
        "cnn_smote.compile(\n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
        ")\n",
        "\n",
        "\n",
        "# Training model\n",
        "cnn_smote.fit(\n",
        "    X_train_smote_cnn,\n",
        "    y_train_smote,\n",
        "    epochs=30,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Predictions\n",
        "y_pred_probs_smote = cnn_smote.predict(X_test_cnn).flatten()\n",
        "y_pred_smote = (y_pred_probs_smote > 0.5).astype(int)\n",
        "\n",
        "# Evaluation\n",
        "cm_smote = confusion_matrix(y_test, y_pred_smote)\n",
        "roc_smote = roc_auc_score(y_test, y_pred_probs_smote)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm_smote)\n",
        "print(f\"AUC: {roc_smote:.4f}\")"
      ],
      "metadata": {
        "id": "Nv4eAfD9A41w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SHAP ANALYSIS FOR CNN SMOTE"
      ],
      "metadata": {
        "id": "dbcncTNfC8eK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract feature names\n",
        "feature_names = X_train.columns.tolist()\n",
        "\n",
        "# Preparing background and test data\n",
        "background = X_train_smote_cnn[np.random.choice(X_train_smote_cnn.shape[0], 100, replace=False)]\n",
        "X_explain = X_test_cnn[np.random.choice(X_test_cnn.shape[0], 50, replace=False)]\n",
        "\n",
        "#Flattening for SHAP KernelExplainer\n",
        "background_flat = background.reshape(background.shape[0], -1)\n",
        "X_explain_flat = X_explain.reshape(X_explain.shape[0], -1)\n",
        "\n",
        "#Defining prediction wrapper for cnn_smote\n",
        "def cnn_smote_predict(x_flat):\n",
        "    x_reshaped = x_flat.reshape((-1, X_train_smote_cnn.shape[1], 1))\n",
        "    return cnn_smote.predict(x_reshaped).flatten()\n",
        "\n",
        "#Creating SHAP explainer\n",
        "explainer_smote = shap.KernelExplainer(cnn_smote_predict, background_flat)\n",
        "\n",
        "#Computing SHAP values\n",
        "shap_values_smote = explainer_smote.shap_values(X_explain_flat, nsamples=\"auto\")\n"
      ],
      "metadata": {
        "id": "0ROMvzjmA44U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SHAP Summary Plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "shap.summary_plot(\n",
        "    shap_values_smote,\n",
        "    X_explain_flat,\n",
        "    feature_names=feature_names,\n",
        "    plot_type=\"dot\",\n",
        "    show=False\n",
        ")\n",
        "plt.title(\"SHAP Summary Plot - CNN (SMOTE)\", fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bCPk9Vx2rJw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ADASYN + CNN"
      ],
      "metadata": {
        "id": "wxH7KBniDhAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying ADASYN\n",
        "adas = ADASYN(random_state=42)\n",
        "X_train_adas, y_train_adas = adas.fit_resample(X_train, y_train)\n",
        "X_train_adas_cnn = X_train_adas.values.reshape((X_train_adas.shape[0], X_train_adas.shape[1], 1))"
      ],
      "metadata": {
        "id": "_5hXupy8C3uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "#CNN architecture for ADASYN\n",
        "cnn_adas = Sequential([\n",
        "    # Block 1\n",
        "    Conv1D(32, 3, activation='relu', padding='same', input_shape=(X_train_adas_cnn.shape[1], 1)),\n",
        "    BatchNormalization(),\n",
        "    Conv1D(64, 3, activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    # Block 2\n",
        "    Conv1D(128, 3, activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Conv1D(128, 3, activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.4),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "lqXTv4NYC3w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling model\n",
        "cnn_adas.compile(\n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
        ")\n",
        "\n",
        "\n",
        "cnn_adas.fit(\n",
        "    X_train_adas_cnn,\n",
        "    y_train_adas,\n",
        "    epochs=30,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Predictions\n",
        "y_pred_probs_adas = cnn_adas.predict(X_test_cnn).flatten()\n",
        "y_pred_adas = (y_pred_probs_adas > 0.5).astype(int)\n",
        "\n",
        "# Evaluation\n",
        "cm_adasyn = confusion_matrix(y_test, y_pred_adas)\n",
        "roc_adasyn = roc_auc_score(y_test, y_pred_probs_adas)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm_adasyn)\n",
        "print(f\"AUC: {roc_adasyn:.4f}\")"
      ],
      "metadata": {
        "id": "WUGRqS5ZC3zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SHAP ANALYSIS FOR ADASYN"
      ],
      "metadata": {
        "id": "ho7MS1FsDuCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract original feature names\n",
        "feature_names = X_train.columns.tolist()  # Ensure X_train is the unreshaped DataFrame\n",
        "\n",
        "# Sample background and test data\n",
        "background = X_train_adas_cnn[np.random.choice(X_train_adas_cnn.shape[0], 100, replace=False)]\n",
        "X_explain = X_test_cnn[np.random.choice(X_test_cnn.shape[0], 50, replace=False)]\n",
        "\n",
        "# Flatten data for SHAP\n",
        "background_flat = background.reshape(background.shape[0], -1)\n",
        "X_explain_flat = X_explain.reshape(X_explain.shape[0], -1)\n",
        "\n",
        "#Prediction wrapper for CNN ADASYN model\n",
        "def cnn_adas_predict(x_flat):\n",
        "    x_reshaped = x_flat.reshape((-1, X_train_adas_cnn.shape[1], 1))\n",
        "    return cnn_adas.predict(x_reshaped).flatten()\n",
        "\n",
        "#Create SHAP KernelExplainer\n",
        "explainer_adas = shap.KernelExplainer(cnn_adas_predict, background_flat)\n",
        "\n",
        "#Compute SHAP values\n",
        "shap_values_adas = explainer_adas.shap_values(X_explain_flat, nsamples=\"auto\")\n"
      ],
      "metadata": {
        "id": "NF6X_g9xC314"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SHAP Summary Plot (Beeswarm)\n",
        "plt.figure(figsize=(6, 4))\n",
        "shap.summary_plot(\n",
        "    shap_values_adas,\n",
        "    X_explain_flat,\n",
        "    feature_names=feature_names,\n",
        "    plot_type=\"dot\",\n",
        "    show=False\n",
        ")\n",
        "plt.title(\"SHAP Summary Plot - CNN (ADASYN)\", fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FgZ25GS6rFvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cost-Sensitive CNN"
      ],
      "metadata": {
        "id": "SSk_J1pDD1uX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing class weights\n",
        "class_weights_array = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights_dict = {0: class_weights_array[0], 1: class_weights_array[1]}"
      ],
      "metadata": {
        "id": "z6lQTHjYC34T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "#CNN architecture for Cost-Sensitive\n",
        "cnn_cost = Sequential([\n",
        "    # Block 1\n",
        "    Conv1D(32, 3, activation='relu', padding='same', input_shape=(X_train_cnn.shape[1], 1)),\n",
        "    BatchNormalization(),\n",
        "    Conv1D(64, 3, activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    # Block 2\n",
        "    Conv1D(128, 3, activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Conv1D(128, 3, activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.4),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "uOJe-Y0UC360"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling with class weights\n",
        "cnn_cost.compile(\n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
        ")\n",
        "\n",
        "\n",
        "# Training with class weights and validation\n",
        "cnn_cost.fit(\n",
        "    X_train_cnn,\n",
        "    y_train,\n",
        "    epochs=30,\n",
        "    batch_size=64,\n",
        "    class_weight=class_weights_dict,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Predictions\n",
        "y_pred_probs_cost = cnn_cost.predict(X_test_cnn).flatten()\n",
        "y_pred_cost = (y_pred_probs_cost > 0.5).astype(int)\n",
        "\n",
        "# Evaluation\n",
        "cm_cost = confusion_matrix(y_test, y_pred_cost)\n",
        "roc_cost = roc_auc_score(y_test, y_pred_probs_cost)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm_cost)\n",
        "print(f\"AUC: {roc_cost:.4f}\")"
      ],
      "metadata": {
        "id": "Rkj_DqQdC39L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SHAP ANALYSIS FOR Cost-Sensitive¶"
      ],
      "metadata": {
        "id": "2GxbHyXwEC3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract original feature names\n",
        "feature_names = X_train.columns.tolist()\n",
        "\n",
        "#Sample background and test data\n",
        "background = X_train_cnn[np.random.choice(X_train_cnn.shape[0], 100, replace=False)]\n",
        "X_explain = X_test_cnn[np.random.choice(X_test_cnn.shape[0], 50, replace=False)]\n",
        "\n",
        "#Flatten data for KernelExplainer\n",
        "background_flat = background.reshape(background.shape[0], -1)\n",
        "X_explain_flat = X_explain.reshape(X_explain.shape[0], -1)\n",
        "\n",
        "#Define prediction wrapper for cost-sensitive CNN model\n",
        "def cnn_cost_predict(x_flat):\n",
        "    x_reshaped = x_flat.reshape((-1, X_train_cnn.shape[1], 1))\n",
        "    return cnn_cost.predict(x_reshaped).flatten()\n",
        "\n",
        "#Create SHAP KernelExplainer\n",
        "explainer_cost = shap.KernelExplainer(cnn_cost_predict, background_flat)\n",
        "\n",
        "#Compute SHAP values\n",
        "shap_values_cost = explainer_cost.shap_values(X_explain_flat, nsamples=\"auto\")\n"
      ],
      "metadata": {
        "id": "MdEP87v1C3_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SHAP Plot\n",
        "plt.figure(figsize=(4, 10))\n",
        "shap.summary_plot(\n",
        "    shap_values_cost,\n",
        "    X_explain_flat,\n",
        "    feature_names=feature_names,\n",
        "    plot_type=\"dot\",\n",
        "    show=False\n",
        ")\n",
        "plt.title(\"SHAP Summary Plot - CNN (Cost-Sensitive)\", fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "K7muV8bxC4B3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot ROC Curves for CNN¶"
      ],
      "metadata": {
        "id": "ZHbstGADEPPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fpr_base, tpr_base, _ = roc_curve(y_test, y_pred_probs_baseline)\n",
        "fpr_smote, tpr_smote, _ = roc_curve(y_test, y_pred_probs_smote)\n",
        "fpr_adas, tpr_adas, _ = roc_curve(y_test, y_pred_probs_adas)\n",
        "fpr_cost, tpr_cost, _ = roc_curve(y_test, y_pred_probs_cost)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(fpr_base, tpr_base, label=\"Baseline\")\n",
        "plt.plot(fpr_smote, tpr_smote, label=\"SMOTE\")\n",
        "plt.plot(fpr_adas, tpr_adas, label=\"ADASYN\")\n",
        "plt.plot(fpr_cost, tpr_cost, label=\"Cost-sensitive\")\n",
        "plt.plot([0,1],[0,1],'k--')\n",
        "plt.title(\"ROC Curves for CNN - Credit Card Fraud\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YdBnakigC4EN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Table"
      ],
      "metadata": {
        "id": "JHe_tOYKETwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Model evaluation table\n",
        "results_cnn = pd.DataFrame({\n",
        "    \"Method\": [\"Baseline\", \"SMOTE\", \"ADASYN\", \"Cost-sensitive\"],\n",
        "    \"Accuracy\": [\n",
        "        accuracy_score(y_test, y_pred_baseline),\n",
        "        accuracy_score(y_test, y_pred_smote),\n",
        "        accuracy_score(y_test, y_pred_adas),\n",
        "        accuracy_score(y_test, y_pred_cost)\n",
        "    ],\n",
        "    \"Precision\": [\n",
        "        precision_score(y_test, y_pred_baseline),\n",
        "        precision_score(y_test, y_pred_smote),\n",
        "        precision_score(y_test, y_pred_adas),\n",
        "        precision_score(y_test, y_pred_cost)\n",
        "    ],\n",
        "    \"Recall\": [\n",
        "        recall_score(y_test, y_pred_baseline),\n",
        "        recall_score(y_test, y_pred_smote),\n",
        "        recall_score(y_test, y_pred_adas),\n",
        "        recall_score(y_test, y_pred_cost)\n",
        "    ],\n",
        "    \"F1\": [\n",
        "        f1_score(y_test, y_pred_baseline),\n",
        "        f1_score(y_test, y_pred_smote),\n",
        "        f1_score(y_test, y_pred_adas),\n",
        "        f1_score(y_test, y_pred_cost)\n",
        "    ],\n",
        "    \"AUC\": [roc_baseline, roc_smote, roc_adasyn, roc_cost]\n",
        "})\n",
        "\n",
        "# View final table\n",
        "print(results_cnn)\n"
      ],
      "metadata": {
        "id": "DykrqazgC4Gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TSvqPYCLC4I7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LSTM MODELS**"
      ],
      "metadata": {
        "id": "lOMJieNLIArJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshaping input\n",
        "X_train_lstm = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test_lstm  = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))"
      ],
      "metadata": {
        "id": "aJXxlWkVC4Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyper-parameter Tuning"
      ],
      "metadata": {
        "id": "5Li76QgrIUpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameter Tuning\n",
        "\n",
        "def build_lstm_model(units, dropout_rate, dense_units, learning_rate):\n",
        "    inter_dropout = max(min(dropout_rate, 0.8), 0.0)\n",
        "    head_dropout  = max(min(dropout_rate + 0.1, 0.8), 0.0)\n",
        "\n",
        "    half_units = max(units // 2, 32)\n",
        "\n",
        "    model = Sequential([\n",
        "        #LSTM Block 1\n",
        "        LSTM(units, return_sequences=True, dropout=inter_dropout, recurrent_dropout=0.1,\n",
        "             input_shape=(X_train_lstm.shape[1], 1)),\n",
        "        BatchNormalization(),\n",
        "\n",
        "        #LSTM Block 2\n",
        "        LSTM(units, return_sequences=True, dropout=inter_dropout, recurrent_dropout=0.1),\n",
        "        BatchNormalization(),\n",
        "\n",
        "        #LSTM Block 3 (final, no return_sequences)\n",
        "        LSTM(half_units, return_sequences=False, dropout=inter_dropout, recurrent_dropout=0.1),\n",
        "        BatchNormalization(),\n",
        "\n",
        "        # Dense head (2 layers)\n",
        "        Dense(dense_units * 2, activation='relu'),\n",
        "        Dropout(head_dropout),\n",
        "        Dense(dense_units, activation='relu'),\n",
        "        Dropout(inter_dropout),\n",
        "\n",
        "        # Output\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Grid definition\n",
        "param_grid_lstm = {\n",
        "    'units': [32, 64, 128, 256],\n",
        "    'dropout_rate': [0.1, 0.2, 0.3, 0.4],\n",
        "    'dense_units': [32,64, 128,256],\n",
        "    'learning_rate': [0.0001,0.0002,0.0003,0.0004,0.0005]\n",
        "}\n",
        "\n",
        "results_lstm = []\n",
        "\n",
        "for params in itertools.product(*param_grid_lstm.values()):\n",
        "    current_params = dict(zip(param_grid_lstm.keys(), params))\n",
        "    print(f\"Testing: {current_params}\")\n",
        "\n",
        "    model = build_lstm_model(**current_params)\n",
        "    history = model.fit(\n",
        "        X_train_lstm,\n",
        "        y_train,\n",
        "        validation_split=0.2,\n",
        "        epochs=10,\n",
        "        batch_size=32,\n",
        "        verbose=0,\n",
        "        callbacks=[early_stop]\n",
        "    )\n",
        "\n",
        "    y_pred_probs = model.predict(X_test_lstm, verbose=0).flatten()\n",
        "    y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    results_lstm.append((current_params, acc))\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "3Xr_c_vvIRJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params_lstm, best_score_lstm = max(results_lstm, key=lambda x: x[1])\n",
        "print(\"\\nBest Parameters for LSTM:\")\n",
        "print(best_params_lstm)\n",
        "print(f\"Best Accuracy: {best_score_lstm:.4f}\")"
      ],
      "metadata": {
        "id": "WVJKBM7-IRNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM BASELINE¶"
      ],
      "metadata": {
        "id": "oK-W10xOIgqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# LSTM model\n",
        "lstm_baseline = Sequential([\n",
        "    # LSTM Block 1\n",
        "    LSTM(128, input_shape=(X_train_lstm.shape[1], 1),\n",
        "         return_sequences=True, dropout=0.3, recurrent_dropout=0.1),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    # LSTM Block 2\n",
        "    LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.1),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    # LSTM Block 3\n",
        "    LSTM(64, return_sequences=False, dropout=0.3, recurrent_dropout=0.1),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    # Dense head\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.4),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "uxPkveODIRQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling\n",
        "lstm_baseline.compile(\n",
        "    optimizer=Adam(learning_rate=0.0005),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
        ")\n",
        "\n",
        "# Fitting model\n",
        "lstm_baseline.fit(\n",
        "    X_train_lstm,\n",
        "    y_train,\n",
        "    epochs=30,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Predicting and evaluation\n",
        "y_pred_probs_baseline_lstm = lstm_baseline.predict(X_test_lstm).flatten()\n",
        "y_pred_baseline_lstm = (y_pred_probs_baseline_lstm > 0.5).astype(int)\n",
        "\n",
        "# Metrics\n",
        "cm_baseline_lstm = confusion_matrix(y_test, y_pred_baseline_lstm)\n",
        "roc_baseline_lstm = roc_auc_score(y_test, y_pred_probs_baseline_lstm)\n",
        "\n",
        "# Results\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm_baseline_lstm)\n",
        "print(f\"AUC: {roc_baseline_lstm:.4f}\")"
      ],
      "metadata": {
        "id": "PfH4ZZQuIRSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SMOTE + LSTM"
      ],
      "metadata": {
        "id": "usFnFD59IsUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_smote_lstm = X_train_smote.values.reshape((X_train_smote.shape[0], X_train_smote.shape[1], 1))"
      ],
      "metadata": {
        "id": "O0OXCAp9IRUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Early stopping\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "lstm_smote = Sequential([\n",
        "    # LSTM Block 1\n",
        "    LSTM(128, input_shape=(X_train_smote_lstm.shape[1], 1),\n",
        "         return_sequences=True, dropout=0.3, recurrent_dropout=0.1),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    # LSTM Block 2\n",
        "    LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.1),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    # LSTM Block 3\n",
        "    LSTM(64, return_sequences=False, dropout=0.3, recurrent_dropout=0.1),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    # Dense head\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.4),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "7EQdwXM9IRXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile with advanced metrics\n",
        "lstm_smote.compile(\n",
        "    optimizer=Adam(learning_rate=0.0005),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
        ")\n",
        "\n",
        "\n",
        "lstm_smote.fit(\n",
        "    X_train_smote_lstm,\n",
        "    y_train_smote,\n",
        "    epochs=30,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Predicting and evaluation\n",
        "y_pred_probs_smote_lstm = lstm_smote.predict(X_test_lstm).flatten()\n",
        "y_pred_smote_lstm = (y_pred_probs_smote_lstm > 0.5).astype(int)\n",
        "\n",
        "cm_smote_lstm = confusion_matrix(y_test, y_pred_smote_lstm)\n",
        "roc_smote_lstm = roc_auc_score(y_test, y_pred_probs_smote_lstm)\n",
        "\n",
        "# Results\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm_smote_lstm)\n",
        "print(f\"AUC: {roc_smote_lstm:.4f}\")"
      ],
      "metadata": {
        "id": "Alj8U_zoIRZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ADASYN + LSTM"
      ],
      "metadata": {
        "id": "uxjP0bQSI3CS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_adas_lstm = X_train_adas.values.reshape((X_train_adas.shape[0], X_train_adas.shape[1], 1))\n",
        "\n",
        "# Early stopping\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# LSTM architecture ADASYN\n",
        "lstm_adas = Sequential([\n",
        "    # LSTM Block 1\n",
        "    LSTM(128, input_shape=(X_train_adas_lstm.shape[1], 1),\n",
        "         return_sequences=True, dropout=0.3, recurrent_dropout=0.1),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    # LSTM Block 2\n",
        "    LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.1),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    # LSTM Block 3\n",
        "    LSTM(64, return_sequences=False, dropout=0.3, recurrent_dropout=0.1),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    # Dense head\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.4),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "vPBQEgsiIRb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling\n",
        "lstm_adas.compile(\n",
        "    optimizer=Adam(learning_rate=0.0005),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
        ")\n",
        "\n",
        "\n",
        "lstm_adas.fit(\n",
        "    X_train_adas_lstm,\n",
        "    y_train_adas,\n",
        "    epochs=30,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "# Predicting and evaluation\n",
        "y_pred_probs_adas_lstm = lstm_adas.predict(X_test_lstm).flatten()\n",
        "y_pred_adas_lstm = (y_pred_probs_adas_lstm > 0.5).astype(int)\n",
        "\n",
        "cm_adasyn_lstm = confusion_matrix(y_test, y_pred_adas_lstm)\n",
        "roc_adasyn_lstm = roc_auc_score(y_test, y_pred_probs_adas_lstm)\n",
        "\n",
        "#Results\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm_adasyn_lstm)\n",
        "print(f\"AUC: {roc_adasyn_lstm:.4f}\")"
      ],
      "metadata": {
        "id": "aZfnkOpzIReV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cost-Sensitive LSTM"
      ],
      "metadata": {
        "id": "PIS0jL44I_cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights_array = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights_dict = {0: class_weights_array[0], 1: class_weights_array[1]}"
      ],
      "metadata": {
        "id": "A0N6NCieIRg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# LSTM architecture Cost-Sensitive\n",
        "lstm_cost = Sequential([\n",
        "    # LSTM Block 1\n",
        "    LSTM(128, input_shape=(X_train_lstm.shape[1], 1),\n",
        "         return_sequences=True, dropout=0.3, recurrent_dropout=0.1),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    # LSTM Block 2\n",
        "    LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.1),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    # LSTM Block 3\n",
        "    LSTM(64, return_sequences=False, dropout=0.3, recurrent_dropout=0.1),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    # Dense head\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.4),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "CZolUJ5gIRjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling model\n",
        "lstm_cost.compile(\n",
        "    optimizer=Adam(learning_rate=0.0005),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
        ")\n",
        "\n",
        "\n",
        "# Training\n",
        "lstm_cost.fit(\n",
        "    X_train_lstm,\n",
        "    y_train,\n",
        "    epochs=30,\n",
        "    batch_size=64,\n",
        "    class_weight=class_weights_dict,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Predicting and evaluation\n",
        "y_pred_probs_cost_lstm = lstm_cost.predict(X_test_lstm).flatten()\n",
        "y_pred_cost_lstm = (y_pred_probs_cost_lstm > 0.5).astype(int)\n",
        "\n",
        "cm_cost_lstm = confusion_matrix(y_test, y_pred_cost_lstm)\n",
        "roc_cost_lstm = roc_auc_score(y_test, y_pred_probs_cost_lstm)\n",
        "\n",
        "# Results\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm_cost_lstm)\n",
        "print(f\"AUC: {roc_cost_lstm:.4f}\")"
      ],
      "metadata": {
        "id": "ChFS-QmzIRlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot ROC Curves for LSTM¶"
      ],
      "metadata": {
        "id": "amYs04WcJLUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fpr_base_lstm, tpr_base_lstm, _ = roc_curve(y_test, y_pred_probs_baseline_lstm)\n",
        "fpr_smote_lstm, tpr_smote_lstm, _ = roc_curve(y_test, y_pred_probs_smote_lstm)\n",
        "fpr_adas_lstm, tpr_adas_lstm, _ = roc_curve(y_test, y_pred_probs_adas_lstm)\n",
        "fpr_cost_lstm, tpr_cost_lstm, _ = roc_curve(y_test, y_pred_probs_cost_lstm)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(fpr_base_lstm, tpr_base_lstm, label=\"Baseline\")\n",
        "plt.plot(fpr_smote_lstm, tpr_smote_lstm, label=\"SMOTE\")\n",
        "plt.plot(fpr_adas_lstm, tpr_adas_lstm, label=\"ADASYN\")\n",
        "plt.plot(fpr_cost_lstm, tpr_cost_lstm, label=\"Cost-sensitive\")\n",
        "plt.plot([0,1],[0,1],'k--')\n",
        "plt.title(\"ROC Curves for LSTM - Credit Card Fraud\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v83eL31JJHTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Metrics Comparison Table"
      ],
      "metadata": {
        "id": "yYmqd2ReJRYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_lstm = pd.DataFrame({\n",
        "    \"Method\": [\"Baseline\", \"SMOTE\", \"ADASYN\", \"Cost-sensitive\"],\n",
        "    \"Accuracy\": [\n",
        "        accuracy_score(y_test, y_pred_baseline_lstm),\n",
        "        accuracy_score(y_test, y_pred_smote_lstm),\n",
        "        accuracy_score(y_test, y_pred_adas_lstm),\n",
        "        accuracy_score(y_test, y_pred_cost_lstm)\n",
        "    ],\n",
        "    \"Precision\": [\n",
        "        precision_score(y_test, y_pred_baseline_lstm),\n",
        "        precision_score(y_test, y_pred_smote_lstm),\n",
        "        precision_score(y_test, y_pred_adas_lstm),\n",
        "        precision_score(y_test, y_pred_cost_lstm)\n",
        "    ],\n",
        "    \"Recall\": [\n",
        "        recall_score(y_test, y_pred_baseline_lstm),\n",
        "        recall_score(y_test, y_pred_smote_lstm),\n",
        "        recall_score(y_test, y_pred_adas_lstm),\n",
        "        recall_score(y_test, y_pred_cost_lstm)\n",
        "    ],\n",
        "    \"F1\": [\n",
        "        f1_score(y_test, y_pred_baseline_lstm),\n",
        "        f1_score(y_test, y_pred_smote_lstm),\n",
        "        f1_score(y_test, y_pred_adas_lstm),\n",
        "        f1_score(y_test, y_pred_cost_lstm)\n",
        "    ],\n",
        "    \"AUC\": [\n",
        "        roc_baseline_lstm,\n",
        "        roc_smote_lstm,\n",
        "        roc_adasyn_lstm,\n",
        "        roc_cost_lstm\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(results_lstm)"
      ],
      "metadata": {
        "id": "S-eLIaMwJHV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HYBRID MODEL(CNN+LSTM)"
      ],
      "metadata": {
        "id": "WJLbu7NAJXrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshaping input\n",
        "X_train_cnnlstm = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test_cnnlstm  = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))"
      ],
      "metadata": {
        "id": "C0O1g2GWJHZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyper-Parameter Tuning"
      ],
      "metadata": {
        "id": "ep9kvfYbJeB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cnn_lstm_model(filters, kernel_size, lstm_units, dropout_rate, dense_units, learning_rate):\n",
        "\n",
        "    block_dropout = min(max(dropout_rate, 0.0), 0.8)\n",
        "    head_dropout  = min(max(dropout_rate + 0.1, 0.0), 0.8)\n",
        "    lstm_units_2  = max(lstm_units // 2, 32)\n",
        "\n",
        "    model = Sequential([\n",
        "        # Conv Block 1\n",
        "        Conv1D(filters=filters, kernel_size=kernel_size, activation='relu',\n",
        "               padding='same', input_shape=(X_train_cnnlstm.shape[1], 1)),\n",
        "        BatchNormalization(),\n",
        "        Conv1D(filters=filters * 2, kernel_size=kernel_size, activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Dropout(block_dropout),\n",
        "\n",
        "        # Conv Block 2\n",
        "        Conv1D(filters=filters * 4, kernel_size=kernel_size, activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Conv1D(filters=filters * 4, kernel_size=kernel_size, activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Dropout(block_dropout),\n",
        "\n",
        "        # LSTM stack\n",
        "        LSTM(lstm_units, return_sequences=True, dropout=block_dropout, recurrent_dropout=0.1),\n",
        "        BatchNormalization(),\n",
        "        LSTM(lstm_units_2, return_sequences=False, dropout=block_dropout, recurrent_dropout=0.1),\n",
        "        BatchNormalization(),\n",
        "\n",
        "        # Dense head\n",
        "        Dense(dense_units, activation='relu'),\n",
        "        Dropout(head_dropout),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Keeping search space narrow for efficiency\n",
        "param_grid_hybrid = {\n",
        "    'filters': [32, 64, 128],\n",
        "    'kernel_size': [3, 4, 5],\n",
        "    'lstm_units': [32, 64, 128],\n",
        "    'dropout_rate': [0.1, 0.2, 0.3, 0.4],\n",
        "    'dense_units': [32, 64, 128, 256],\n",
        "    'learning_rate': [0.0001,0.0002, 0.0003, 0.0004,0.0005]\n",
        "}\n",
        "\n",
        "results_hybrid = []\n",
        "\n",
        "for params in itertools.product(*param_grid_hybrid.values()):\n",
        "    current_params = dict(zip(param_grid_hybrid.keys(), params))\n",
        "    print(f\"Testing: {current_params}\")\n",
        "\n",
        "    model = build_cnn_lstm_model(**current_params)\n",
        "    history = model.fit(\n",
        "        X_train_cnnlstm,\n",
        "        y_train,\n",
        "        validation_split=0.2,\n",
        "        epochs=30,\n",
        "        batch_size=32,\n",
        "        verbose=0,\n",
        "        callbacks=[early_stop]\n",
        "    )\n",
        "\n",
        "    y_pred_probs = model.predict(X_test_cnnlstm, verbose=0).flatten()\n",
        "    y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    results_hybrid.append((current_params, acc))\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "\n",
        "best_params_hybrid, best_score_hybrid = max(results_hybrid, key=lambda x: x[1])\n",
        "print(\"\\nBest Parameters for CNN+LSTM:\")\n",
        "print(best_params_hybrid)\n",
        "print(f\"Best Accuracy: {best_score_hybrid:.4f}\")"
      ],
      "metadata": {
        "id": "DGZpDDRNJHa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hybrid (CNN + LSTM) Baseline"
      ],
      "metadata": {
        "id": "sonmD7rZJtUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# CNN+LSTM model\n",
        "cnn_lstm_baseline = Sequential([\n",
        "    # Conv Block 1\n",
        "    Conv1D(32, 3, activation='relu', padding='same', input_shape=(X_train_cnnlstm.shape[1], 1)),\n",
        "    BatchNormalization(),\n",
        "    Conv1D(64, 3, activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    # Conv Block 2\n",
        "    Conv1D(128, 3, activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Conv1D(128, 3, activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    # LSTM stack\n",
        "    LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.1),\n",
        "    BatchNormalization(),\n",
        "    LSTM(64, return_sequences=False, dropout=0.3, recurrent_dropout=0.1),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    # Dense head\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.4),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "mGvnnbzMJHft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "cnn_lstm_baseline.compile(\n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
        ")\n",
        "\n",
        "\n",
        "# Fitting with early stopping\n",
        "cnn_lstm_baseline.fit(\n",
        "    X_train_cnnlstm,\n",
        "    y_train,\n",
        "    epochs=30,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Predicting and evaluating\n",
        "y_pred_probs_baseline_cnnlstm = cnn_lstm_baseline.predict(X_test_cnnlstm).flatten()\n",
        "y_pred_baseline_cnnlstm = (y_pred_probs_baseline_cnnlstm > 0.5).astype(int)\n",
        "\n",
        "cm_baseline_cnnlstm = confusion_matrix(y_test, y_pred_baseline_cnnlstm)\n",
        "roc_baseline_cnnlstm = roc_auc_score(y_test, y_pred_probs_baseline_cnnlstm)\n",
        "\n",
        "# Results\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm_baseline_cnnlstm)\n",
        "print(f\"AUC: {roc_baseline_cnnlstm:.4f}\")"
      ],
      "metadata": {
        "id": "icKKMugPJHiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SMOTE + CNN + LSTM"
      ],
      "metadata": {
        "id": "nqzY1XrlJ779"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_smote_cnnlstm = X_train_smote.values.reshape((X_train_smote.shape[0], X_train_smote.shape[1], 1))"
      ],
      "metadata": {
        "id": "I71Zx73OJHkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Hybrid CNN+LSTM model for SMOTE\n",
        "cnn_lstm_smote = Sequential([\n",
        "    # Conv Block 1\n",
        "    Conv1D(32, 3, activation='relu', padding='same', input_shape=(X_train_smote_cnnlstm.shape[1], 1)),\n",
        "    BatchNormalization(),\n",
        "    Conv1D(64, 3, activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(2),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    # Conv Block 2\n",
        "    Conv1D(128, 3, activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Conv1D(128, 3, activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(2),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    # LSTM stack\n",
        "    LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.1),\n",
        "    BatchNormalization(),\n",
        "    LSTM(64, return_sequences=False, dropout=0.3, recurrent_dropout=0.1),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    # Dense head\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.4),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "ZJgnXNRnJHmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling model\n",
        "cnn_lstm_smote.compile(\n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
        ")\n",
        "\n",
        "\n",
        "# Fitting model\n",
        "cnn_lstm_smote.fit(\n",
        "    X_train_smote_cnnlstm,\n",
        "    y_train_smote,\n",
        "    epochs=30,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Predicting and evaluating\n",
        "y_pred_probs_smote_cnnlstm = cnn_lstm_smote.predict(X_test_cnnlstm).flatten()\n",
        "y_pred_smote_cnnlstm = (y_pred_probs_smote_cnnlstm > 0.5).astype(int)\n",
        "\n",
        "cm_smote_cnnlstm = confusion_matrix(y_test, y_pred_smote_cnnlstm)\n",
        "roc_smote_cnnlstm = roc_auc_score(y_test, y_pred_probs_smote_cnnlstm)\n",
        "\n",
        "# Output\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm_smote_cnnlstm)\n",
        "print(f\"AUC: {roc_smote_cnnlstm:.4f}\")"
      ],
      "metadata": {
        "id": "QZn8mVxHJHpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ADASYN + CNN + LSTM¶"
      ],
      "metadata": {
        "id": "mBFUujQwKJp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_adas_cnnlstm = X_train_adas.values.reshape((X_train_adas.shape[0], X_train_adas.shape[1], 1))"
      ],
      "metadata": {
        "id": "gdv4qUMoJHrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Hybrid CNN+LSTM Architecture for ADASYN\n",
        "cnn_lstm_adas = Sequential([\n",
        "    # Conv Block 1\n",
        "    Conv1D(32, 3, activation='relu', padding='same', input_shape=(X_train_adas_cnnlstm.shape[1], 1)),\n",
        "    BatchNormalization(),\n",
        "    Conv1D(64, 3, activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(2),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    # Conv Block 2\n",
        "    Conv1D(128, 3, activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Conv1D(128, 3, activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(2),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    # LSTM stack\n",
        "    LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.1),\n",
        "    BatchNormalization(),\n",
        "    LSTM(64, return_sequences=False, dropout=0.3, recurrent_dropout=0.1),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    # Dense head\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.4),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "n88ev3pNJHtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling model\n",
        "cnn_lstm_adas.compile(\n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
        ")\n",
        "\n",
        "# Fitting model\n",
        "cnn_lstm_adas.fit(\n",
        "    X_train_adas_cnnlstm,\n",
        "    y_train_adas,\n",
        "    epochs=30,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Predicting and evaluating\n",
        "y_pred_probs_adas_cnnlstm = cnn_lstm_adas.predict(X_test_cnnlstm).flatten()\n",
        "y_pred_adas_cnnlstm = (y_pred_probs_adas_cnnlstm > 0.5).astype(int)\n",
        "\n",
        "cm_adasyn_cnnlstm = confusion_matrix(y_test, y_pred_adas_cnnlstm)\n",
        "roc_adasyn_cnnlstm = roc_auc_score(y_test, y_pred_probs_adas_cnnlstm)\n",
        "\n",
        "# Output\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm_adasyn_cnnlstm)\n",
        "print(f\"AUC: {roc_adasyn_cnnlstm:.4f}\")"
      ],
      "metadata": {
        "id": "Qe0jAQtdJHwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cost-Sensitive CNN + LSTM¶"
      ],
      "metadata": {
        "id": "ldZ46PuWKUf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# CNN+LSTM model\n",
        "cnn_lstm_cost = Sequential([\n",
        "    # Conv Block 1\n",
        "    Conv1D(32, 3, activation='relu', padding='same', input_shape=(X_train_cnnlstm.shape[1], 1)),\n",
        "    BatchNormalization(),\n",
        "    Conv1D(64, 3, activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(2),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    # Conv Block 2\n",
        "    Conv1D(128, 3, activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    Conv1D(128, 3, activation='relu', padding='same'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling1D(2),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    # LSTM stack\n",
        "    LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.1),\n",
        "    BatchNormalization(),\n",
        "    LSTM(64, return_sequences=False, dropout=0.3, recurrent_dropout=0.1),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    # Dense head\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.4),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "AEkFRMWwKRQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling model\n",
        "cnn_lstm_cost.compile(\n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', Precision(name='precision'), Recall(name='recall')]\n",
        ")\n",
        "\n",
        "# Training model\n",
        "cnn_lstm_cost.fit(\n",
        "    X_train_cnnlstm,\n",
        "    y_train,\n",
        "    epochs=100,\n",
        "    batch_size=64,\n",
        "    class_weight=class_weights_dict,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Predictions and Evaluation\n",
        "y_pred_probs_cost_cnnlstm = cnn_lstm_cost.predict(X_test_cnnlstm).flatten()\n",
        "y_pred_cost_cnnlstm = (y_pred_probs_cost_cnnlstm > 0.5).astype(int)\n",
        "\n",
        "cm_cost_cnnlstm = confusion_matrix(y_test, y_pred_cost_cnnlstm)\n",
        "roc_cost_cnnlstm = roc_auc_score(y_test, y_pred_probs_cost_cnnlstm)\n",
        "\n",
        "# Output\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm_cost_cnnlstm)\n",
        "print(f\"AUC: {roc_cost_cnnlstm:.4f}\")"
      ],
      "metadata": {
        "id": "Xvw3zzz9KRSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot ROC Curves for CNN + LSTM¶"
      ],
      "metadata": {
        "id": "F4xS1eaXKdTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fpr_base_cnnlstm, tpr_base_cnnlstm, _ = roc_curve(y_test, y_pred_probs_baseline_cnnlstm)\n",
        "fpr_smote_cnnlstm, tpr_smote_cnnlstm, _ = roc_curve(y_test, y_pred_probs_smote_cnnlstm)\n",
        "fpr_adas_cnnlstm, tpr_adas_cnnlstm, _ = roc_curve(y_test, y_pred_probs_adas_cnnlstm)\n",
        "fpr_cost_cnnlstm, tpr_cost_cnnlstm, _ = roc_curve(y_test, y_pred_probs_cost_cnnlstm)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(fpr_base_cnnlstm, tpr_base_cnnlstm, label=\"Baseline\")\n",
        "plt.plot(fpr_smote_cnnlstm, tpr_smote_cnnlstm, label=\"SMOTE\")\n",
        "plt.plot(fpr_adas_cnnlstm, tpr_adas_cnnlstm, label=\"ADASYN\")\n",
        "plt.plot(fpr_cost_cnnlstm, tpr_cost_cnnlstm, label=\"Cost-sensitive\")\n",
        "plt.plot([0,1],[0,1],'k--')\n",
        "plt.title(\"ROC Curves for CNN + LSTM - Credit Card Fraud\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bBjFQ4_bKRUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Metrics Comparison Table¶"
      ],
      "metadata": {
        "id": "Z7SYWgfVKil9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_cnnlstm = pd.DataFrame({\n",
        "    \"Method\": [\"Baseline\", \"SMOTE\", \"ADASYN\", \"Cost-sensitive\"],\n",
        "    \"Accuracy\": [\n",
        "        accuracy_score(y_test, y_pred_baseline_cnnlstm),\n",
        "        accuracy_score(y_test, y_pred_smote_cnnlstm),\n",
        "        accuracy_score(y_test, y_pred_adas_cnnlstm),\n",
        "        accuracy_score(y_test, y_pred_cost_cnnlstm)\n",
        "    ],\n",
        "    \"Precision\": [\n",
        "        precision_score(y_test, y_pred_baseline_cnnlstm),\n",
        "        precision_score(y_test, y_pred_smote_cnnlstm),\n",
        "        precision_score(y_test, y_pred_adas_cnnlstm),\n",
        "        precision_score(y_test, y_pred_cost_cnnlstm)\n",
        "    ],\n",
        "    \"Recall\": [\n",
        "        recall_score(y_test, y_pred_baseline_cnnlstm),\n",
        "        recall_score(y_test, y_pred_smote_cnnlstm),\n",
        "        recall_score(y_test, y_pred_adas_cnnlstm),\n",
        "        recall_score(y_test, y_pred_cost_cnnlstm)\n",
        "    ],\n",
        "    \"F1\": [\n",
        "        f1_score(y_test, y_pred_baseline_cnnlstm),\n",
        "        f1_score(y_test, y_pred_smote_cnnlstm),\n",
        "        f1_score(y_test, y_pred_adas_cnnlstm),\n",
        "        f1_score(y_test, y_pred_cost_cnnlstm)\n",
        "    ],\n",
        "    \"AUC\": [\n",
        "        roc_baseline_cnnlstm,\n",
        "        roc_smote_cnnlstm,\n",
        "        roc_adasyn_cnnlstm,\n",
        "        roc_cost_cnnlstm\n",
        "    ]\n",
        "})\n",
        "print(results_cnnlstm)"
      ],
      "metadata": {
        "id": "TP-s345aKRXa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}